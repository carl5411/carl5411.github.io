<!DOCTYPE html>
<!-- test -->
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project - GorillaChow | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Tailored Music Recommendations from Natural Language Prompts</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">GorillaChow</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <!-- <div class="author-image">
                        
              <img src="">
            
            
          </div> -->
          <p>
                        
              Isaac Blumhoefer
            
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
            
            <img src="">
            
          </div> -->
          <p>
            
            Elijah Carlson
            
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
            
              <img src="">            
            
          </div> -->
          <p>
              Lucas Harrison
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
                        
              <img src="">
            
          </div> -->
          <p>
            Charles Hart
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span> -->
          <!-- Demo link -->
          <span class="link-block">
            <a
              href="https://pages.github.umn.edu/harr2512/gorillachow_demo/"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Try the Demo</span>
            </a>
          </span>
          <!-- Github Link-->
          <span class="link-block">
            <a
              href="https://github.umn.edu/Hart1299/CSCI5541-Project/tree/main"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          <!-- </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>               -->
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>
  We wanted to create a novel application that would recommend songs and music playlists based on user prompts. 
  While Spotify came out with a similar feature recently, we wanted to create an application that more readily understands the user prompt. 
  We currently use MU-LLaMA to decipher the music audio in our datasets and extract musical features. 
  We then tried using the Gemini API to interpret the human prompts to extract similar features from that, 
  and we also gathered some feedback to help train the model. 
  We do not have full results yet, however we have considerable progress in each aspect of the model.
</p>
  

<hr>

<p class="sys-img"><img src="./files/images/bettergorrilachowdiagram.png" alt="imgname"></p>


<!-- <h3 id="the-timeline-and-the-highlights">Any subsection</h3> -->

<p>A basic representation of how our application will prosses data to give results.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What we are trying to do</b>
</p>
<p>
  We want to create an application that would recommend songs and music playlists based on user prompts, 
  wanting to create a more personalized and tailored experience for the user.
  We also wanted to discover what kind of music our application thinks very abstract prompts would sound like. 
  For example, what would a song sound like if it was based on the prompt "a song that sounds like a sunset"?
  Our goal is to create something that is not affected by the user's listening history 
  and is more based on the prompt itself while being able to handle more abstract prompts and hopefully return sensible songs.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  Spotify has a new feature that recommends songs and playlists based on user listening history and song titles. 
  However, the recommendations that Spotify gives are not very in-depth, 
  and a lot of the recommendations are based on the user's listening history and song titles.
  Because of this, the songs they provide are not going to "catch the vibe" as it were, of the prompt given if the prompt is too abstract.
  There have also been plenty of studies looking into music recommendation systems, 
  but there are still a few gaps in the research.
  For example, there is not much work on handling content-based prompts. 
  Similarly, the integration of lyrical sentiment analysis with audio feature analysis has not been fully explored.
</p>
<p>
  Obviously, with very abstract prompts it is extremely hard if not impossible to say whether or not a song is a good recommendation.
  Thus, there is a limit as to what can be expected from a music recommendation system. 
  Furthermore, music is a very subjective medium, and because of this people will not like the songs that are recommended to them, while others may.
  It seems highly unlikely that we will see a music recommendation system that can perfectly predict what a user will like and wants in our lifetime.
  Lastly, there are an absurd number of songs within the human music library 
  making it very difficult to create a model that can handle that much data.
</p>
</p>

<p>
<b>Why should you care?</b>
</p>
<p>
  If we succeed, we will have created a music recommendation system that can handle abstract prompts and return sensible song suggestions.
  This could help people discover new music that they would not have found otherwise, 
  along with aiding people to create playlists for moments in their lives with more ease.
  Our application will hopefully be able to capture more complex and metaphorical ideas than current music recommendation systems, 
  and thus be able to recommend songs that are more in line with the user's intent.
  This will open up new possibilities for how people listen to music, 
  allowing unfamiliar songs and artists to be discovered based on emotions and environment rather than just listening history.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  Our application was created with a simple enough pipeline in mind. 
  First, we take the prompt given by the user and use a model to derive musical qualities and descriptors from it.
  Then we take the parsed descriptors and map them onto both audio a lyrical data using a separate model which extracts similar qualities 
  and descriptors from the music dataset.
  We currently have two options for this, either using human feedback to train the model or using an LLM to output scores based on the human data.
  The LLM we are currently looking at is Gemini, which is capable of parsing features such as acousticness, danceability, etc. from the prompt given.
  Below is a diagram showing how this process works.
</p>
<img alt="" src="./files/images/GeminiPromptDiagram.png">
<p>
We thought this would be successful as integrating both lyrical and audio features would give us a more complete picture of the music, 
and thus a better recommendation. 
We did this using a MU-LLaMA (Music Understanding Large Language Model) model. Below is a diagram showing how this process works.
</p>
<img alt="" src="./files/images/AudioEngineeringDiagram.png">
<p>
  MU-LLaMA is a tool that can be used to extract musical features from audio files (specifically .wav files), 
  while also being able to analyze lyrics in songs. 
  The model works by feeding it an input .wav file and giving it a prompt asking for audio analysis, as shown in the diagram. 
  As for handling abstract prompts, by parsing natural language into musical features, 
  our application will be able to more readily understand the intent of the prompt.

</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
  So far we have not encountered any major problems. One thing we did have to consider was how to train the model that handles the user input.
  We need a model to parse musical features from the prompt, and we have two options. 
  First, we could use scores from human feedback to solely train the model, which would be more efficient given the smaller scope of this project.
  Otherwise, we could create an LLM such as Gemini to output scores based on the human data. 
  This would be more flexible and allow for more complex prompts, but would also be more time-consuming.
  We have, of course, through the planning of this project worried about one main problem: music is subjective, and thus it is extremely hard to validate results.
  This being said, since we have our own metric scores along with Spotify's dataset, we should be able to come up with a reasonably effective metric.
  Using human feedback instead of the Spotify dataset is looking to be the way we will go at the moment.
  Either way, we will be trying both options in the hope of finding the best solution.
  For audio engineering, MU-LLaMA is extremely effective, however does have some limitations.
  For example, when given audio from popular meme culture, the model depicts a sad violin playing, given it has no cultural context. 
  We have also tried to obtain song lyrics from datasets but found this to not be feasible, so web scraping seems to be our only option.
</p>

<hr>
    
<h2 id="results">Preliminary Results</h2>
<!-- <p> -->
<!-- <b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b> -->

<!-- </p> -->
<p>
  Before we had a working demo we had some preliminary results from various parts of our application.
  As for the audio parsing model, we had an experimental model using MU-LLaMA working that can obtain musical features from .wav files. 
  (This is described in more detail in the Approach section.)
  For the human prompt, we had a working Gemini model that is capable of parsing features such as acoustics, danceability, etc. from the prompt given.
  This model is capable of handling abstract prompts, however, we were not able to verify the correctness of these results at the time.
  Lastly, a survey was conducted to gather human feedback on prompt features using the same tags as the Gemini model.
  These are the results discussed in the previous section about issues.
</p>
<table class="table">
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompt</strong></th>
      <th style="text-align: center">Number of responses</th>
      <th style="text-align: center">Acoustic Mean</th>

      <th style="text-align: center">Danceability Mean</th>

      <th style="text-align: center">Energy Mean</th>

      <th style="text-align: center">Instrumentalness Mean</th>

      <th style="text-align: center">Liveness Mean</th>

      <th style="text-align: center">Valence Mean</th>

    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Songs for a sunny day</td>
      <td>5</td>
      <td>6.2</td>
      <td>8.4</td>
      <td>8.4</td>
      <td>3.8</td>
      <td>3.8</td>
      <td>9.6</td>
    </tr>
    <tr>
      <td>Music to study to</td>
      <td>5</td>
      <td>7.0</td>
      <td>2.2</td>
      <td>4.0</td>
      <td>8.4</td>
      <td>2.2</td>
      <td>5.2</td>
    </tr>
    <tr>
      <td>Songs for a party</td>
      <td>5</td>
      <td>2.2</td>
      <td>10.0</td>
      <td>10.0</td>
      <td>2.2</td>
      <td>2.0</td>
      <td>9.2</td>
    </tr>
    <tr>
      <td>Chill Music Before Bed</td>
      <td>5</td>
      <td>7.8</td>
      <td>3.2</td>
      <td>2.0</td>
      <td>6.4</td>
      <td>3.0</td>
      <td>4.4</td>
    </tr>
    <tr>
      <td>Workout playlist</td>
      <td>5</td>
      <td>2.2</td>
      <td>9.6</td>
      <td>10.0</td>
      <td>2.4</td>
      <td>2.0</td>
      <td>8.2</td>
    </tr>
    <tr>
      <td>Romantic Dinner Songs</td>
      <td>11</td>
      <td>7.3</td>
      <td>5.0</td>
      <td>3.4</td>
      <td>4.6</td>
      <td>3.7</td>
      <td>7.4</td>
    </tr>
    <tr>
      <td>Sad Love Songs</td>
      <td>11</td>
      <td>7.3</td>
      <td>2.0</td>
      <td>2.5</td>
      <td>4.5</td>
      <td>2.9</td>
      <td>1.1</td>
    </tr>
    <tr>
      <td>Winter Music</td>
      <td>11</td>
      <td>6.5</td>
      <td>4.3</td>
      <td>5.3</td>
      <td>5.7</td>
      <td>3.7</td>
      <td>6.5</td>
    </tr>
    <tr>
      <td>Songs for a rainy day</td>
      <td>11</td>
      <td>7.2</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>6.0</td>
      <td>4.9</td>
      <td>3.5</td>
    </tr>
    <tr>
      <td>Jazz Classics</td>
      <td>11</td>
      <td>7.6</td>
      <td>7.7</td>
      <td>7.4</td>
      <td>8.9</td>
      <td>6.9</td>
      <td>6.4</td>
    </tr>
    <tr>
      <td>Backyard barbecue</td>
      <td>7</td>
      <td>3.6</td>
      <td>4.9</td>
      <td>6.1</td>
      <td>3.4</td>
      <td>3.7</td>
      <td>9.1</td>
    </tr>
    <tr>
      <td>Nostalgic hits</td>
      <td>7</td>
      <td>3.9</td>
      <td>7.6</td>
      <td>7.7</td>
      <td>3.2</td>
      <td>2.3</td>
      <td>8.0</td>
    </tr>
    <tr>
      <td>Fall songs</td>
      <td>7</td>
      <td>6.9</td>
      <td>3.3</td>
      <td>4.3</td>
      <td>5.9</td>
      <td>4.1</td>
      <td>4.1</td>
    </tr>
    <tr>
      <td>Late night driving</td>
      <td>7</td>
      <td>5.3</td>
      <td>5.3</td>
      <td>6.0</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>5.4</td>
    </tr>
    <tr>
      <td>Good vibes pop</td>
      <td>7</td>
      <td>2.7</td>
      <td>8.6</td>
      <td>8.1</td>
      <td>2.9</td>
      <td>2.6</td>
      <td>9.7</td>
    </tr>
    <tr>
      <td>Pregame jams</td>
      <td>6</td>
      <td>2.7</td>
      <td>9.3</td>
      <td>9.3</td>
      <td>3.2</td>
      <td>2.3</td>
      <td>9.7</td>
    </tr>
    <tr>
      <td>Sleepy music</td>
      <td>6</td>
      <td>8.0</td>
      <td>1.7</td>
      <td>1.7</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>5.3</td>
    </tr>
    <tr>
      <td>Spring hits</td>
      <td>6</td>
      <td>5.8</td>
      <td>6.3</td>
      <td>7.0</td>
      <td>3.7</td>
      <td>5.5</td>
      <td>7.7</td>
    </tr>
    <tr>
      <td>Calm focus playlist</td>
      <td>6</td>
      <td>7.8</td>
      <td>1.5</td>
      <td>2.8</td>
      <td>7.8</td>
      <td>1.8</td>
      <td>5.5</td>
    </tr>
    <tr>
      <td>Sleepless midnight</td>
      <td>6</td>
      <td>5.3</td>
      <td>4.7</td>
      <td>4.7</td>
      <td>4.3</td>
      <td>4.3</td>
      <td>4.0</td>
    </tr>
  </tbody>
  <caption>Audio prompt analysis based on peer responses</caption>
</table>
<br>

<p>Here we have the comparisons from the peer responses and the Gemini Outputs.</p>
<div style="text-align: center;">
<!-- <img style="height: 300px;" alt="" src="./files/results.png"> -->
<img alt="" src="./files/images/Acousticness_Comparison.png">
<img alt="" src="./files/images/Danceability_Comparison.png">
<img alt="" src="./files/images/Energy_Comparison.png">
<img alt="" src="./files/images/Instrumental_Comparison.png">
<img alt="" src="./files/images/Liveness_Comparison.png">
<img alt="" src="./files/images/Valance_Comparison.png">

</div>
<br><br>

<hr>

<h2 id="results">Results</h2>
<!-- <p> -->
<!-- <b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b> -->

<!-- </p> -->
<p>
  fudkd
</p>

<h2 id="conclusion">Conclusion and Future Work</h2>
<p>
 Because our application is limited in scope, an eventual increase in the dataset of recommendable songs would be a good next step.
 This would allow for more accurate recommendations and would make the application more useful.
 Another big thing that we would like to implement is to have more user feedback to on the model's accuracy.
 This would allow us to improve the model in all aspects, from the prompt mood analysis to each song's given features. 
 Overall, however, we are very happy with the progress we have made so far, with the working demo and the results we have gathered from it.
 Those results are also fairly easy to reproduce, as the demo is available to anyone who wants to try it. 
 All of the code used to create the demo is on our GitHub page, so anyone can see how we created the application, and can 
 easily reproduce the model should they want to. 


  <!-- How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research? -->

</p>

<hr>


  </div>
  


</body></html>
